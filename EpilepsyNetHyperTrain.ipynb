{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import importlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "import EpilepsyNet_model \n",
    "from preprocessing_utils import *\n",
    "\n",
    "epoch_duration = 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 : Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('eeg_metadata.xlsx')\n",
    "df['epilepsy'] = df['edf_path'].apply(lambda x: 0 if 'no_epilepsy' in x else 1)\n",
    "try:\n",
    "    df.drop(['Unnamed: 0', 'ethnicity'], axis=1, inplace=True)\n",
    "except:\n",
    "    pass\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make balanced Train/test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Each label contains 100 patients (with different number & times of acquisition.\n",
    "    The epilepic patients have on average more recordings (closer attention on them).\n",
    "    However the most important is to avoid data leakage and split the datasets on patient ids.\n",
    "'''\n",
    "\n",
    "df_pos = df[df['epilepsy'] == 1]\n",
    "df_neg = df[df['epilepsy'] == 0]\n",
    "\n",
    "print(df_pos['subject_id'].nunique(), df_neg['subject_id'].nunique())\n",
    "split_pos = int(0.8 * len(df_pos['subject_id'].unique()))\n",
    "print(split_pos)\n",
    "\n",
    "split_train_pos, split_test_pos = df_pos['subject_id'].unique()[:split_pos], df_pos['subject_id'].unique()[split_pos:]\n",
    "\n",
    "print(len(split_train_pos ), len(split_test_pos ))\n",
    "##############################\n",
    "split_neg = int(0.8 * len(df_neg['subject_id'].unique()))\n",
    "print(split_neg)\n",
    "\n",
    "split_train_neg, split_test_neg = df_neg['subject_id'].unique()[:split_neg], df_neg['subject_id'].unique()[split_neg:]\n",
    "\n",
    "print(len(split_train_neg), len(split_test_neg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_pos[df_pos['subject_id'].isin(split_train_pos)], df_neg[df_neg['subject_id'].isin(split_train_neg)]])\n",
    "print(df_train.shape[0], df_train['epilepsy'].value_counts())\n",
    "display(df_train)\n",
    "\n",
    "df_test = pd.concat([df_pos[df_pos['subject_id'].isin(split_test_pos)], df_neg[df_neg['subject_id'].isin(split_test_neg)]])\n",
    "print(df_test.shape[0], df_test['epilepsy'].value_counts())\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list, y_train = Load_raw_labeled(\n",
    "                                        df_train.groupby('subject_id').nth(range(25))\n",
    "                                    )\n",
    "X_test_list , y_test  = Load_raw_labeled(\n",
    "                                        df_test.groupby('subject_id').nth(range(25))\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train_list), sum(y_train), len(y_train), len(X_test_list), sum(y_test), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 : Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation : 1 minute split into 12 segments of 5 secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EEG channels used for prediction\n",
    "eeg_cols = ['EEG FP1', 'EEG FP2', 'EEG F3', 'EEG F4', \n",
    "            'EEG C3', 'EEG C4', 'EEG P3', 'EEG P4', \n",
    "            'EEG O1', 'EEG O2', 'EEG F7', 'EEG F8', \n",
    "            'EEG T3', 'EEG T4', 'EEG T5', 'EEG T6', \n",
    "            'EEG T1', 'EEG T2', 'EEG FZ', 'EEG CZ',\n",
    "            'EEG PZ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process raw files to get tensor of shape (len(raw_files), 12, len(eeg_cols), 1250)\n",
    "X_train = process_raw_files(\n",
    "    X_train_list,\n",
    "    eeg_cols=eeg_cols,\n",
    "    segment_duration=60.0,        # 60 second segments\n",
    "    n_segments_per_file=12,       # Split into 12 epochs (5 sec each)\n",
    "    samples_per_segment=1250,     # 1250 samples per segment (250 Hz sampling rate)\n",
    "    random_state=42               # For reproducibility\n",
    ")\n",
    "\n",
    "X_test = process_raw_files(\n",
    "    X_test_list,\n",
    "    eeg_cols=eeg_cols,\n",
    "    segment_duration=60.0,        \n",
    "    n_segments_per_file=12,       \n",
    "    samples_per_segment=1250,     \n",
    "    random_state=42               \n",
    ")\n",
    "\n",
    "print(f\"Output shape: train->{X_train.shape}\")\n",
    "# Expected shape: (len(raw_files), 12, 21, 1250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Output shape: train->{X_train.shape}, test->{X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check element of X_train that are all 0 :\n",
    "zero_elements_train = np.where(np.all(X_train == 0, axis=(1, 2, 3)))\n",
    "print(f\"Number of all-zero elements in train: {len(zero_elements_train[0])}\")\n",
    "# Check element of X_test that are all 0 :\n",
    "zero_elements_test = np.where(np.all(X_test == 0, axis=(1, 2, 3)))\n",
    "print(f\"Number of all-zero elements in test: {len(zero_elements_test[0])}\")\n",
    "\n",
    "# Drop all-zero elements from X_train and y_train\n",
    "X_train = np.delete(X_train, zero_elements_train[0], axis=0)\n",
    "y_train = np.delete(y_train, zero_elements_train[0], axis=0)\n",
    "# Drop all-zero elements from X_test and y_test\n",
    "X_test = np.delete(X_test, zero_elements_test[0], axis=0)\n",
    "y_test = np.delete(y_test, zero_elements_test[0], axis=0)\n",
    "print(f\"Output shape: train->{X_train.shape}, test->{X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standardization -> Correlation Matrices -> flattening upper triangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "X_train_standardized = standardize_data(X_train)\n",
    "print(f\"Standardized output shape: {X_train_standardized.shape}\")\n",
    "# Expected shape: (len(raw_files), 12, 21, 1250)\n",
    "# Compute the correlation matrix per sample, segment, and channel\n",
    "\n",
    "corr_matrix_train = compute_correlation_matrix(X_train_standardized)\n",
    "print(f\"Correlation matrix shape: {corr_matrix_train.shape}\")\n",
    "# Expected shape: (21, 21)\n",
    "# Display the correlation matrix\n",
    "plt.figure(figsize=(10, 8)) \n",
    "plt.imshow(corr_matrix_train[0][11], cmap='coolwarm', aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.title('Correlation Matrix')\n",
    "plt.xlabel('Channels')\n",
    "plt.ylabel('Channels')\n",
    "plt.xticks(range(len(eeg_cols)), eeg_cols, rotation=90)\n",
    "plt.yticks(range(len(eeg_cols)), eeg_cols)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "upper_triangle_matrix_train = extract_upper_triangle(corr_matrix_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "X_test_standardized = standardize_data(X_test)\n",
    "print(f\"Standardized output shape: {X_test_standardized.shape}\")\n",
    "print(f\"# of nan values : {np.isnan(X_test_standardized).sum()}\")\n",
    "# Expected shape: (len(raw_files), 12, 21, 1250)\n",
    "# Compute the correlation matrix per sample, segment, and channel\n",
    "\n",
    "corr_matrix_test = compute_correlation_matrix(X_test_standardized)\n",
    "print(f\"Correlation matrix shape: {corr_matrix_test.shape}\")\n",
    "print(f\"# of nan values : {np.isnan(corr_matrix_test).sum()}\")\n",
    "# Expected shape: (21, 21)\n",
    "\n",
    "# Example usage\n",
    "upper_triangle_matrix_test = extract_upper_triangle(corr_matrix_test)\n",
    "\n",
    "print(upper_triangle_matrix_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(upper_triangle_matrix_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "# Create TensorDataset\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True )\n",
    "\n",
    "X_test_tensor = torch.tensor(upper_triangle_matrix_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "# Create TensorDataset\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "# Create DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_tensor.shape)\n",
    "print(X_test_tensor.shape)\n",
    "print(y_train_tensor.shape)\n",
    "print(y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 : Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(EpilepsyNet_model)\n",
    "\n",
    "# Model parameters\n",
    "input_dim = 210  # Size of flattened upper triangle (21*20/2)\n",
    "embed_dim = 56  # Embedding dimension\n",
    "num_heads = 7    # Number of attention heads\n",
    "\n",
    "model = EpilepsyNet_model.TimeSeriesAttentionClassifier(input_dim, embed_dim, num_heads, dropout=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses, val_accuracies = EpilepsyNet_model.train_model(\n",
    "    model, \n",
    "    train_loader, \n",
    "    test_loader,\n",
    "    num_epochs=1000,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=1e-5,\n",
    "    patience=20,\n",
    "    scheduler_factor=0.5,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_accuracies)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy (%)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize attention for a sample\n",
    "# visualize_attention(model, X_standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model locally:\n",
    "torch.save(model.state_dict(), 'Weights_model/EpilepsyNet_7Heads.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=np.arange(1, 150),\n",
    "    y=train_losses,\n",
    "    mode='lines+markers',\n",
    "    name='Train Loss'\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=np.arange(1, 150),\n",
    "    y=val_losses,\n",
    "    mode='lines+markers',\n",
    "    name='Validation Loss'\n",
    "))\n",
    "fig.update_layout(\n",
    "    title='Training and Validation Loss',\n",
    "    xaxis_title='Epoch',\n",
    "    yaxis_title='Loss',\n",
    "    legend_title='Legend'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "y_pred, _ = model(torch.tensor(next(iter(test_loader))))#, dtype=torch.float32)).argmax(dim=1).numpy()\n",
    "metrics = calculate_metrics(y_test, y_pred)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to inference : matrix list attention score (210x210) giving correlation p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load th model:\n",
    "model = EpilepsyNet_model.TimeSeriesAttentionClassifier(input_dim, embed_dim, num_heads)\n",
    "model.load_state_dict(torch.load('Weights_model/EpilepsyNet.pth'))\n",
    "model.eval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
